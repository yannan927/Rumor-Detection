{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kCsdsk8l7sF"
   },
   "outputs": [],
   "source": [
    "! kill -9 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVnlcN6gmEeb"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iX9lDzoTmGBb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"There are {} GPUs available.\".format(torch.cuda.device_count()))\n",
    "    print(\"We will use GPU {}\".format(torch.cuda.get_device_name(0)))\n",
    "else:\n",
    "    print(\"There is no GPU available, using the CPU instead!\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fm5mrlirmHp5"
   },
   "source": [
    "读取文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6DTcJOEmJ6c"
   },
   "outputs": [],
   "source": [
    "# Read the source and the reply into different lists\n",
    "\n",
    "def read_file(text_file_name):\n",
    "  ### 输入为train文件的路径，本函数将tweet原始text内容及reply text内容进行分开存放;\n",
    "  \n",
    "    original_s_text = []\n",
    "    original_r_text = []\n",
    "    original_ids = []\n",
    "\n",
    "    count = 0\n",
    "    with open(text_file_name, 'r') as f:\n",
    "        for line in f:\n",
    "          count += 1\n",
    "          content = json.loads(line)\n",
    "\n",
    "          original_ids.append(content[0]['id_str'])\n",
    "          s_text_str = \"\"\n",
    "          r_text_str = \"\"\n",
    "          for item in content:\n",
    "              if not item[\"in_reply_to_user_id_str\"]:\n",
    "                s_text_str = s_text_str + \" \" + item['text']   #字符串合并\n",
    "              else:\n",
    "                r_text_str = r_text_str + \" \" + item['text']\n",
    "          \n",
    "          original_s_text.append(s_text_str)\n",
    "          original_r_text.append(r_text_str)\n",
    "\n",
    "        print(\"There are {} events\".format(count))\n",
    "\n",
    "    return original_s_text, original_r_text, original_ids\n",
    "\n",
    "train_file_name = 'train.data.jsonl'\n",
    "train_tweet_s_texts, train_tweet_r_texts, train_ids = read_file(train_file_name)\n",
    "\n",
    "print(len(train_tweet_s_texts))\n",
    "print(len(train_tweet_r_texts))\n",
    "print(len(train_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwQSMpcrnpQH"
   },
   "outputs": [],
   "source": [
    "train_tweet_s_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3haGowLIntJG"
   },
   "outputs": [],
   "source": [
    "train_tweet_r_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZl48wffnxkW"
   },
   "outputs": [],
   "source": [
    "#remove URL#\n",
    "def remove_urls (text):\n",
    "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "#remove @XXX#\n",
    "def remove_user(text):\n",
    "    text = re.sub('@[^\\s]*','', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "# print(remove_user(train_tweet_texts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbdppFbXn1qq"
   },
   "outputs": [],
   "source": [
    "puns=[\",\",\".\",\"?\",\"!\"]\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def pre_processing(tweet_texts):\n",
    "    pre_processed_tweet = []\n",
    "    for tweet in tweet_texts:\n",
    "        re_tweet = remove_urls(tweet)\n",
    "        u_re_tweet = remove_user(re_tweet)\n",
    "        tokens = tokenizer.tokenize(u_re_tweet)\n",
    "        text_str = \"\"\n",
    "        for token in tokens:\n",
    "            if token in string.punctuation and token not in puns:\n",
    "                continue\n",
    "            elif not token.isspace():\n",
    "              text_str = text_str + \" \" + token\n",
    "        pre_processed_tweet.append(text_str.strip())\n",
    "    \n",
    "    return pre_processed_tweet\n",
    "\n",
    "train_processed_s_texts = pre_processing(train_tweet_s_texts)\n",
    "train_processed_r_texts = pre_processing(train_tweet_r_texts)\n",
    "\n",
    "print(len(train_processed_s_texts))\n",
    "print(len(train_processed_r_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LASiN4XAn1t1"
   },
   "outputs": [],
   "source": [
    "train_processed_s_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cViAtO-8n1uo"
   },
   "outputs": [],
   "source": [
    "train_processed_r_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPkp8R1Jn1wd"
   },
   "outputs": [],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "859PcSg2n1yG"
   },
   "outputs": [],
   "source": [
    "# Loading the Bertweet Tokeinzer\n",
    "\n",
    "from transformers import BertweetTokenizer\n",
    "\n",
    "print(\"Loading the Bertweet Tokenizer!\")\n",
    "Bertweet_tokenizer = BertweetTokenizer.from_pretrained('vinai/bertweet-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2pWXR935vbE"
   },
   "outputs": [],
   "source": [
    "! pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65kCLZlDoWxJ"
   },
   "outputs": [],
   "source": [
    "def print_max_len(processed_texts):\n",
    "    tweet_len = []\n",
    "    for text in processed_texts:\n",
    "        text_len = len(Bertweet_tokenizer.tokenize(text))\n",
    "        tweet_len.append(text_len)\n",
    "    \n",
    "    print('The maximum length is {}'.format(max(tweet_len)))\n",
    "    return tweet_len\n",
    "\n",
    "tweets_s_len = print_max_len(train_processed_s_texts)\n",
    "tweets_r_len = print_max_len(train_processed_r_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVpb8QrOoy1H"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    " \n",
    "\n",
    "plt.bar(range(len(tweets_s_len)), tweets_s_len, color='rgb')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKHnOu4Yo3bB"
   },
   "outputs": [],
   "source": [
    "plt.bar(range(len(tweets_r_len)), tweets_r_len, color='rgb')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "teXoV9pgo8Qj"
   },
   "outputs": [],
   "source": [
    "print('The Average Length of Source Tweet is {}'.format(np.mean(tweets_s_len)))\n",
    "print('The Median Length of Source Tweet is {}'.format(np.median(tweets_s_len)))\n",
    "print('The 80 percentile of Length of Source Length is {}'.format(np.quantile(tweets_s_len, 0.8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGIb4arMo8lo"
   },
   "outputs": [],
   "source": [
    "print('The Average Length of Reply Tweet is {}'.format(np.mean(tweets_r_len)))\n",
    "print('The Median Length of Reply Tweet is {}'.format(np.median(tweets_r_len)))\n",
    "print('The 80 percentile of Length of Reply Length is {}'.format(np.quantile(tweets_r_len, 0.8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qC6uDwuRpVFG"
   },
   "outputs": [],
   "source": [
    "# Using Bertweet Tokenzier\n",
    "\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "def convert_para_to_id(contents, para_length):\n",
    "    input_ids_list = []\n",
    "    attentions_list = []\n",
    "    for content in contents:\n",
    "        encoded_con = Bertweet_tokenizer.encode_plus(content,\n",
    "                                            truncation=True,\n",
    "                                            add_special_tokens=True,\n",
    "                                            max_length = para_length, \n",
    "                                            pad_to_max_length=True,\n",
    "                                            return_tensors = 'pt')\n",
    "        input_ids_list.append(encoded_con[\"input_ids\"])\n",
    "        attentions_list.append(encoded_con[\"attention_mask\"])\n",
    "    return input_ids_list, attentions_list\n",
    "\n",
    "train_s_ids_list, train_s_att_list = convert_para_to_id(train_processed_s_texts, MAX_LENGTH)\n",
    "train_r_ids_list, train_r_att_list = convert_para_to_id(train_processed_r_texts, MAX_LENGTH)\n",
    "\n",
    "print(len(train_s_ids_list))\n",
    "print(len(train_r_ids_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKA1_7MdpsBV"
   },
   "outputs": [],
   "source": [
    "# Read the Validation Data\n",
    "dev_file_name = 'dev.data.jsonl'\n",
    "dev_tweet_s_texts,dev_tweet_r_texts, dev_ids = read_file(dev_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7rbAlLvp0tU"
   },
   "outputs": [],
   "source": [
    "dev_preprocessed_s_texts = pre_processing(dev_tweet_s_texts)\n",
    "\n",
    "print(len(dev_preprocessed_s_texts))\n",
    "print(dev_preprocessed_s_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iVs3bRYp0vl"
   },
   "outputs": [],
   "source": [
    "dev_preprocessed_r_texts = pre_processing(dev_tweet_r_texts)\n",
    "\n",
    "print(len(dev_preprocessed_r_texts))\n",
    "print(dev_preprocessed_r_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwyXXTuBp0yB"
   },
   "outputs": [],
   "source": [
    "dev_s_ids_list, dev_s_att_list = convert_para_to_id(dev_preprocessed_s_texts, MAX_LENGTH)\n",
    "dev_r_ids_list, dev_r_att_list = convert_para_to_id(dev_preprocessed_r_texts, MAX_LENGTH)\n",
    "\n",
    "print(len(dev_s_ids_list))\n",
    "print(len(dev_r_ids_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-kxA0y9p00R"
   },
   "outputs": [],
   "source": [
    "# Now Encode the label\n",
    "\n",
    "def load_label_file(label_file):\n",
    "    with open(label_file, 'r') as file:\n",
    "         labels = json.load(file)\n",
    "         return labels\n",
    "\n",
    "def labels_to_vec(labels, train_ids):\n",
    "    label_list = []\n",
    "    for id in train_ids:\n",
    "        label_list.append(labels[id])\n",
    "    le = LabelEncoder()\n",
    "    vec = le.fit_transform(label_list)\n",
    "\n",
    "    return vec\n",
    "\n",
    "def get_label_vec(label_file, ids):\n",
    "    labels = load_label_file(label_file)\n",
    "    vecs = labels_to_vec(labels, ids)\n",
    "\n",
    "    return vecs\n",
    "\n",
    "train_labels_vec = get_label_vec(\"train.label.json\", train_ids)\n",
    "dev_labels_vec = get_label_vec('dev.label.json', dev_ids)\n",
    "\n",
    "print(train_labels_vec.shape)\n",
    "print(dev_labels_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oxhp1DZRqeyQ"
   },
   "source": [
    "Now Transform All Data into Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2Ih9Fx-p03Z"
   },
   "outputs": [],
   "source": [
    "def redefine_label_vec(labels_vec):\n",
    "    vec_list = []\n",
    "    for i in labels_vec:\n",
    "        a = [0, 0]\n",
    "        a[i] = 1\n",
    "        vec_list.append(a)\n",
    "    return vec_list\n",
    "\n",
    "train_labels_re_vec = redefine_label_vec(train_labels_vec)\n",
    "dev_labels_re_vec = redefine_label_vec(dev_labels_vec)\n",
    "\n",
    "print(len(train_labels_re_vec))\n",
    "print(len(dev_labels_re_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjPqDNi8p05o"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "train_s_input_ids = torch.cat(train_s_ids_list, dim=0)\n",
    "train_s_att_masks = torch.cat(train_s_att_list, dim=0)\n",
    "train_r_input_ids = torch.cat(train_r_ids_list, dim=0)\n",
    "train_r_att_masks = torch.cat(train_r_att_list, dim=0)\n",
    "train_labels = torch.Tensor(train_labels_re_vec)\n",
    "\n",
    "train_data = TensorDataset(train_s_input_ids, train_s_att_masks, train_r_input_ids, train_r_att_masks, train_labels)\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "train_loader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeTMVVRxp070"
   },
   "outputs": [],
   "source": [
    "val_s_input_ids = torch.cat(dev_s_ids_list, dim=0)\n",
    "val_s_att_masks = torch.cat(dev_s_att_list, dim=0)\n",
    "val_r_input_ids = torch.cat(dev_r_ids_list, dim=0)\n",
    "val_r_att_masks = torch.cat(dev_r_att_list, dim=0)\n",
    "val_labels = torch.Tensor(dev_labels_re_vec)\n",
    "\n",
    "\n",
    "val_data = TensorDataset(val_s_input_ids, val_s_att_masks, val_r_input_ids, val_r_att_masks, val_labels)\n",
    "\n",
    "val_loader = DataLoader(val_data, shuffle=False, batch_size=batch_size)\n",
    "print(len(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNy22Hutp1tJ"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AdamW\n",
    "\n",
    "\n",
    "class RumourDetector(nn.Module):\n",
    "    def __init__(self, sent_length, input_dim, kernel_size, hidden_dim, output_dim, dropout):\n",
    "       super().__init__()\n",
    "       self.bertweet = AutoModel.from_pretrained(\"vinai/bertweet-base\")\n",
    "       self.ConvEncoderLayer_1 = nn.Conv1d(in_channels=input_dim, out_channels=input_dim,\n",
    "                                           kernel_size=kernel_size)\n",
    "      #  self.ConvEncoderLayer_2 = nn.Conv1d(in_channels=input_dim, out_channels=input_dim,\n",
    "      #                                      kernel_size=kernel_size[1])\n",
    "       \n",
    "       self.maxpool = nn.MaxPool1d(kernel_size= sent_length- kernel_size + 1)\n",
    "       self.encoder = nn.Linear(2*input_dim, 2*hidden_dim)\n",
    "       self.predictor = nn.Linear(2*hidden_dim, output_dim)\n",
    "       self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, s_input_ids, s_attention_mask, r_input_ids, r_attention_mask):\n",
    "        s_bertweetlayer = self.bertweet(s_input_ids, attention_mask=s_attention_mask)\n",
    "        r_bertweetlayer = self.bertweet(r_input_ids, attention_mask=r_attention_mask)\n",
    "\n",
    "        # concat_bertweetlayer = torch.cat((s_bertweetlayer[0], r_bertweetlayer[0]), 1)\n",
    "        s_bertweet_tensor = s_bertweetlayer[0].permute(0, 2, 1)\n",
    "        conv_bertweet_s_layer = self.ConvEncoderLayer_1(s_bertweet_tensor)\n",
    "        # conv_bertweet_layer2 = self.ConvEncoderLayer_2(conv_bertweet_layer1)\n",
    "        s_pooled_tensor = self.maxpool(conv_bertweet_s_layer)\n",
    "        s_pooled_tensor = s_pooled_tensor.squeeze(2)\n",
    "\n",
    "        r_bertweet_tensor = r_bertweetlayer[0].permute(0, 2, 1)\n",
    "        conv_bertweet_r_layer = self.ConvEncoderLayer_1(r_bertweet_tensor)\n",
    "        r_pooled_tensor = self.maxpool(conv_bertweet_r_layer)\n",
    "        r_pooled_tensor = r_pooled_tensor.squeeze(2)\n",
    "\n",
    "        pooled_tensor = torch.cat((s_pooled_tensor, r_pooled_tensor), 1)\n",
    "        output = self.encoder(pooled_tensor)\n",
    "        output = self.dropout(output)\n",
    "        result = self.predictor(output)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITSyChrjp1vd"
   },
   "outputs": [],
   "source": [
    "model = RumourDetector(128, 768, 3, 768, 2, 0.1)\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6aQPPwWup1xu"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "learning_rate = 2e-5\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = learning_rate, eps=1e-8)\n",
    "print(\"Optimizer Loading Completed!...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TPf8zBpp10I"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hd6Yx8aQ0x6f"
   },
   "outputs": [],
   "source": [
    "def calculate_F1_score(preds, y):\n",
    "    true_label = y.argmax(dim=1)\n",
    "    preds = preds.argmax(dim=1)\n",
    "\n",
    "    tp = (true_label * preds).sum().to(torch.float32)\n",
    "    tn = ((1 - true_label) * (1 - preds)).sum().to(torch.float32)\n",
    "    fp = ((1 - true_label) * preds).sum().to(torch.float32)\n",
    "    fn = (true_label * (1 - preds)).sum().to(torch.float32)\n",
    "    \n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    precision = tp / (tp + fp + epsilon)\n",
    "    recall = tp / (tp + fn + epsilon)\n",
    "    \n",
    "    f1 = 2* (precision*recall) / (precision + recall + epsilon) \n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-mwf9cb0yCI"
   },
   "outputs": [],
   "source": [
    "# Now training\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "epochs = 20\n",
    "best_valid_score = 0\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "\n",
    "training_stats = [] # used to store the training information\n",
    "\n",
    "\n",
    "for epoch_i in range(epochs):\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"======== Epoch {:} / {:} ========\".format(epoch_i + 1, epochs))\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    total_train_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for step, batch in enumerate(train_loader):\n",
    "        \n",
    "        if (step + 1) % 10 == 0 and not step == 0:\n",
    "            print(\"Batch {} of {}\".format(step+1, len(train_loader)))\n",
    "        \n",
    "        batch_s_input_ids = batch[0].cuda()\n",
    "        batch_s_input_mask = batch[1].cuda()\n",
    "        batch_r_input_ids = batch[2].cuda()\n",
    "        batch_r_input_mask = batch[3].cuda()\n",
    "        batch_labels = batch[4].cuda()\n",
    "\n",
    "        model.zero_grad() \n",
    "        preds = model(batch_s_input_ids, batch_s_input_mask, batch_r_input_ids, batch_r_input_mask)\n",
    "        loss = criterion(preds, batch_labels)\n",
    "        loss = loss.float()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\" Average Training Loss is {:2f}\".format(avg_train_loss))\n",
    "    \n",
    "    # Now perform validation\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_eval_score = 0\n",
    "    total_eval_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    for batch in val_loader:\n",
    "\n",
    "\n",
    "        val_s_input_ids = batch[0].cuda()\n",
    "        val_s_input_mask = batch[1].cuda()\n",
    "        val_r_input_ids = batch[2].cuda()\n",
    "        val_r_input_mask = batch[3].cuda()\n",
    "        val_labels = batch[4].cuda()\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_preds = model(val_s_input_ids, val_s_input_mask, val_r_input_ids, val_r_input_mask)\n",
    "        loss = criterion(val_preds, val_labels)\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        total_eval_score += calculate_F1_score(val_preds, val_labels)\n",
    "        \n",
    "    avg_val_loss = total_eval_loss / len(val_loader)\n",
    "    avg_val_score = total_eval_score / len(val_loader)\n",
    "\n",
    "    print(\"Validation loss :{}\".format(avg_val_loss))\n",
    "    print(\"The Score is {}\".format(avg_val_score))\n",
    "\n",
    "    if avg_val_score > best_valid_score:\n",
    "        best_valid_score = avg_val_score\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "    \n",
    "\n",
    "print(\"\")\n",
    "print(\"Training Complete!...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttjqYuycddMs"
   },
   "source": [
    "Now we make prediction on the test dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3I519fgU4cvn"
   },
   "outputs": [],
   "source": [
    "test_file_name = 'test.data.jsonl'\n",
    "\n",
    "def read_file(text_file_name):\n",
    "  ### 输入为train文件的路径，本函数将tweet原始text内容及reply text内容进行分开存放;\n",
    "  \n",
    "    original_s_text = []\n",
    "    original_r_text = []\n",
    "    original_ids = []\n",
    "\n",
    "    count = 0\n",
    "    with open(text_file_name, 'r') as f:\n",
    "        for line in f:\n",
    "          count += 1\n",
    "          content = json.loads(line)\n",
    "\n",
    "          original_ids.append(content[0]['id_str'])\n",
    "          s_text_str = \"\"\n",
    "          r_text_str = \"\"\n",
    "          for item in content:\n",
    "              if not item[\"in_reply_to_user_id_str\"]:\n",
    "                s_text_str = s_text_str + \" \" + item['text']   #字符串合并\n",
    "              else:\n",
    "                r_text_str = r_text_str + \" \" + item['text']\n",
    "          \n",
    "          original_s_text.append(s_text_str)\n",
    "          original_r_text.append(r_text_str)\n",
    "\n",
    "        print(\"There are {} events\".format(count))\n",
    "\n",
    "    return original_s_text, original_r_text, original_ids\n",
    "\n",
    "\n",
    "puns=[\",\",\".\",\"?\",\"!\"]\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "def pre_processing(tweet_texts):\n",
    "    pre_processed_tweet = []\n",
    "    for tweet in tweet_texts:\n",
    "        re_tweet = remove_urls(tweet)\n",
    "        u_re_tweet = remove_user(re_tweet)\n",
    "        tokens = tokenizer.tokenize(u_re_tweet)\n",
    "        text_str = \"\"\n",
    "        for token in tokens:\n",
    "            if token in string.punctuation and token not in puns:\n",
    "                continue\n",
    "            elif not token.isspace():\n",
    "              text_str = text_str + \" \" + token\n",
    "        pre_processed_tweet.append(text_str.strip())\n",
    "    \n",
    "    return pre_processed_tweet\n",
    "\n",
    "\n",
    "test_tweet_s_texts,test_tweet_r_texts, test_ids = read_file(test_file_name)\n",
    "\n",
    "test_preprocessed_s_texts = pre_processing(test_tweet_s_texts)\n",
    "test_preprocessed_r_texts = pre_processing(test_tweet_r_texts)\n",
    "print(len(test_preprocessed_s_texts))\n",
    "print(len(test_preprocessed_r_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aoiICz3iVLmR"
   },
   "outputs": [],
   "source": [
    "def convert_para_to_id(contents, para_length):\n",
    "    input_ids_list = []\n",
    "    attentions_list = []\n",
    "    for content in contents:\n",
    "        encoded_con = Bertweet_tokenizer.encode_plus(content,\n",
    "                                            truncation=True,\n",
    "                                            add_special_tokens=True,\n",
    "                                            max_length = para_length, \n",
    "                                            pad_to_max_length=True,\n",
    "                                            return_tensors = 'pt')\n",
    "        input_ids_list.append(encoded_con[\"input_ids\"])\n",
    "        attentions_list.append(encoded_con[\"attention_mask\"])\n",
    "    return input_ids_list, attentions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tTvABgvh1j3U"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "MAX_LENGTH = 128\n",
    "test_s_ids_list, test_s_att_list = convert_para_to_id(test_preprocessed_s_texts, MAX_LENGTH)\n",
    "test_r_ids_list, test_r_att_list = convert_para_to_id(test_preprocessed_r_texts, MAX_LENGTH)\n",
    "\n",
    "test_s_input_ids = torch.cat(test_s_ids_list, dim=0)\n",
    "test_s_att_masks = torch.cat(test_s_att_list, dim=0)\n",
    "test_r_input_ids = torch.cat(test_r_ids_list, dim=0)\n",
    "test_r_att_masks = torch.cat(test_r_att_list, dim=0)\n",
    "\n",
    "test_batch_size = 83\n",
    "test_data = TensorDataset(test_s_input_ids, test_s_att_masks, test_r_input_ids, test_r_att_masks)\n",
    "\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=test_batch_size)\n",
    "print(len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TGiAsfvR1j56"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model.pt'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPNqNQee1j8C"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "pred_labels = []\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in test_loader:\n",
    "        test_s_input_ids = batch[0].cuda()\n",
    "        test_s_input_att_masks = batch[1].cuda()\n",
    "        test_r_input_ids = batch[2].cuda()\n",
    "        test_r_input_att_masks = batch[3].cuda()\n",
    "\n",
    "        outputs = model(test_s_input_ids, test_s_input_att_masks, test_r_input_ids, test_r_input_att_masks)\n",
    "        outputs = outputs.argmax(dim=1)\n",
    "        preds = outputs.detach().cpu().numpy()\n",
    "        for label in preds:\n",
    "            pred_labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmHjHz_XXQg0"
   },
   "outputs": [],
   "source": [
    "print(len(pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LWcXMJKWqXb"
   },
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "for label in pred_labels:\n",
    "    if label == 0:\n",
    "       test_labels.append('non-rumour')\n",
    "    else:\n",
    "      test_labels.append('rumour')\n",
    "\n",
    "print(len(test_labels))\n",
    "\n",
    "test_dict = {}\n",
    "for i in range(len(test_ids)):\n",
    "    test_dict[test_ids[i]] = test_labels[i]\n",
    "\n",
    "print(len(test_dict))\n",
    "\n",
    "json_str = json.dumps(test_dict)\n",
    "with open ('test-output.json', 'w') as json_file:\n",
    "    json_file.write(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M87XQyR7vUfK"
   },
   "source": [
    " Now we Perform Analysis on COVID-19 Tweet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fc2BQZ-Zvrio"
   },
   "outputs": [],
   "source": [
    "covid_file_name = 'covid.data.jsonl'\n",
    "\n",
    "\n",
    "covid_tweet_s_texts,covid_tweet_r_texts, covid_ids = read_file(covid_file_name)\n",
    "\n",
    "covid_preprocessed_s_texts = pre_processing(covid_tweet_s_texts)\n",
    "covid_preprocessed_r_texts = pre_processing(covid_tweet_r_texts)\n",
    "\n",
    "covid_s_ids_list, covid_s_att_list = convert_para_to_id(covid_preprocessed_s_texts, MAX_LENGTH)\n",
    "covid_r_ids_list, covid_r_att_list = convert_para_to_id(covid_preprocessed_r_texts, MAX_LENGTH)\n",
    "print(len(covid_preprocessed_s_texts))\n",
    "print(len(covid_preprocessed_r_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdnjDsMdYUXC"
   },
   "outputs": [],
   "source": [
    "covid_s_input_ids = torch.cat(covid_s_ids_list, dim=0)\n",
    "covid_s_att_masks = torch.cat(covid_s_att_list, dim=0)\n",
    "covid_r_input_ids = torch.cat(covid_r_ids_list, dim=0)\n",
    "covid_r_att_masks = torch.cat(covid_r_att_list, dim=0)\n",
    "\n",
    "covid_batch_size = \n",
    "covid_data = TensorDataset(covid_s_input_ids, covid_s_att_masks, covid_r_input_ids, covid_r_att_masks)\n",
    "\n",
    "covid_loader = DataLoader(covid_data, shuffle=False, batch_size=covid_batch_size)\n",
    "print(len(covid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBriwsBp-2Co"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in covid_loader:\n",
    "        covid_s_input_ids = batch[0]\n",
    "        covid_s_input_att_masks = batch[1]\n",
    "        covid_r_input_ids = batch[2]\n",
    "        covid_r_input_att_masks = batch[3]\n",
    "\n",
    "        outputs = model(covid_s_input_ids, covid_s_input_att_masks, covid_r_input_ids, covid_r_input_att_masks)\n",
    "        outputs = outputs.argmax(dim=1)\n",
    "        preds = outputs.detach().cpu().numpy()\n",
    "\n",
    "covid_labels = []\n",
    "for label in preds:\n",
    "    if label == 0:\n",
    "       covid_labels.append('non-rumour')\n",
    "    else:\n",
    "      covid_labels.append('rumour')\n",
    "\n",
    "print(len(covid_labels))\n",
    "\n",
    "covid_dict = {}\n",
    "for i in range(len(covid_ids)):\n",
    "    covid_dict[covid_ids[i]] = covid_labels[i]\n",
    "\n",
    "print(len(covid_dict))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
